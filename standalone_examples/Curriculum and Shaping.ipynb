{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "insured-allen",
   "metadata": {},
   "source": [
    "<div id=top></div>\n",
    "\n",
    "# Reinforcement Learning with Doom - Reward shaping and curriculum learning\n",
    "\n",
    "Leandro Kieliger\n",
    "contact@lkieliger.ch\n",
    "\n",
    "---\n",
    "## Description\n",
    "\n",
    "In this notebook we are going to significantly improve the learning efficiency of the setup created in the previous part of this series. First, we will see how to modify rewards to incentivize behaviors helping reach the initial goal, a method called \"reward shaping\". In the second part, we will design an adaptive learning process that varies the difficulty of the training environment based on the performance of the agent. \n",
    "\n",
    "\n",
    "### [Part 1 - Reward Shaping](#part_1)\n",
    "* [Action multipliers](#shaping_table)\n",
    "* [Shaped environment wrapper](#shaped_env)\n",
    "\n",
    "    \n",
    "### [Part 2 - Curriculum Learning](#part_2)\n",
    "* [ACS script](#acs_script)\n",
    "* [Curriculum environment wrapper](#curriculum_env)\n",
    "* [Final model](#final_model)\n",
    "    \n",
    "    \n",
    "### [Bonus - Human vs AI, playing against a trained agent](#bonus)\n",
    "\n",
    "### [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-parameter",
   "metadata": {},
   "source": [
    "<div id=part_1></div>\n",
    "\n",
    "# [^](#top) Part 1 - Reward Shaping\n",
    "\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "comparative-demand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import typing as t\n",
    "import vizdoom\n",
    "from stable_baselines3 import ppo\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common import evaluation, policies\n",
    "from torch import nn\n",
    "\n",
    "from common import envs, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-belle",
   "metadata": {},
   "source": [
    "In the previous notebook we saw that the learning process was very slow. Indeed, even after training more than 2 million steps, our agent barely reached 2 frags per match on average. In comparison, the best bot manages to get around 13 frags. For reference, here is the average performance of six consecutive runs. The shaded area shows the mean error.\n",
    "\n",
    "![Comparison performance](./figures/comparison_shaping_1.png)\n",
    "\n",
    "We also discussed one of the main reason why the model had so much difficulties getting started. The issue is related to rewards being sparse. That is, the agent has to execute many steps \"just right\" before it can observe some meaningful reward signal. It must manage to move and aim at ennemies while repeatedly shooting them in order to (possibly) get some rewards. Such sequence of action rarely happens by chance. If rewards are rare, this means that it will take a long time to reinforce good behaviors.\n",
    "\n",
    "<div id=shaping_table></div>\n",
    "\n",
    "## Action multipliers\n",
    "\n",
    "To solve the issue of sparse rewards, we can give our agent small positive rewards for every action we believe will be beneficial to the learning process. Here is the list of actions we would like to incentivize as well as the associated reward:\n",
    "\n",
    "| Action                     | Reward       |\n",
    "| -------------------------- |--------------| \n",
    "| Frag                       |  1 per frag   | \n",
    "| Damaging enemies           |  0.01 per damage point | \n",
    "| Picking up ammunition      |  0.02 per unit |\n",
    "| Using ammunition           | -0.01 per unit | \n",
    "| Picking up health          |  0.02 per health point |\n",
    "| Losing health              | -0.01 per health point |\n",
    "| Picking up armor           |  0.01 per armor point |\n",
    "| Moved distance > 3 units   |  5e-5 per step |\n",
    "| Moved distance < 3 units   | -2.5e-3 per step |\n",
    "\n",
    "Note that players typically have 100 health points and that damage points correspond to the number of ennemy health points that were removed. Also, players can typically move at around 16 units per tick. The distance reward is here to avoid \"camping\" behavior. Values have been inspired and adapted from a paper using this technique to improve their performance. [Wu, Yuxin and Yuandong Tian. “Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning.” ICLR (2017).](https://research.fb.com/wp-content/uploads/2017/04/paper_camera_ready_small-1.pdf)\n",
    "\n",
    "To modify the rewards we just need to keep track of a few variables and adapt the value before passing it on to the agent. First we set the action rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "younger-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "# 1 per kill\n",
    "reward_factor_frag = 1.0\n",
    "reward_factor_damage = 0.01\n",
    "\n",
    "# Player can move at ~16.66 units per tick\n",
    "reward_factor_distance = 0.00005\n",
    "penalty_factor_distance = 0.0025\n",
    "reward_threshold_distance = 3.0\n",
    "\n",
    "# Pistol clips have 10 bullets\n",
    "reward_factor_ammo_increment = 0.02\n",
    "reward_factor_ammo_decrement = -0.01\n",
    "\n",
    "# Player starts at 100 health\n",
    "reward_factor_health_increment = 0.02\n",
    "reward_factor_health_decrement = -0.01\n",
    "reward_factor_armor_increment = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-entrance",
   "metadata": {},
   "source": [
    "<div id=shaped_env></div>\n",
    "\n",
    "## Shaped environment wrapper\n",
    "Then, we define a game environment wrapper class, just like the one with bots we did in the previous part. It might seem long at first but most of the code is actually computing each reward component based on the multipliers we defined above. Each component is aggregated in the `shape_rewards` function when performing a `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eleven-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "from gym import spaces\n",
    "from vizdoom.vizdoom import GameVariable\n",
    "\n",
    "from common.envs import DoomWithBots\n",
    "\n",
    "# List of game variables storing ammunition information. Used for keeping track of ammunition-related rewards.\n",
    "AMMO_VARIABLES = [GameVariable.AMMO0, GameVariable.AMMO1, GameVariable.AMMO2, GameVariable.AMMO3, GameVariable.AMMO4,\n",
    "                  GameVariable.AMMO5, GameVariable.AMMO6, GameVariable.AMMO7, GameVariable.AMMO8, GameVariable.AMMO9]\n",
    "\n",
    "# List of game variables storing weapon information. Used for keeping track of ammunition-related rewards.\n",
    "WEAPON_VARIABLES = [GameVariable.WEAPON0, GameVariable.WEAPON1, GameVariable.WEAPON2, GameVariable.WEAPON3,\n",
    "                    GameVariable.WEAPON4,\n",
    "                    GameVariable.WEAPON5, GameVariable.WEAPON6, GameVariable.WEAPON7, GameVariable.WEAPON8,\n",
    "                    GameVariable.WEAPON9]\n",
    "\n",
    "class DoomWithBotsShaped(DoomWithBots):\n",
    "    \"\"\"An environment wrapper for a Doom deathmatch game with bots. \n",
    "    \n",
    "    Rewards are shaped according to the multipliers defined in the notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, frame_processor, frame_skip, n_bots, shaping):\n",
    "        super().__init__(game, frame_processor, frame_skip, n_bots)\n",
    "\n",
    "        # Give a random two-letter name to the agent identifying instances in parallel learning.\n",
    "        self.name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=2))\n",
    "        self.shaping = shaping\n",
    "\n",
    "        # Internal states\n",
    "        self.total_rew = 0\n",
    "        self.last_damage_dealt = 0\n",
    "        self.deaths = 0\n",
    "        self.last_frags = 0\n",
    "        self.last_health = 100\n",
    "        self.last_armor = 0\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.ammo_state = self._get_ammo_state()\n",
    "        self.weapon_state = self._get_weapon_state()\n",
    "\n",
    "        # Store individual reward contributions for logging purposes\n",
    "        self.rewards_stats = {\n",
    "            'frag': 0,\n",
    "            'damage': 0,\n",
    "            'ammo': 0,\n",
    "            'health': 0,\n",
    "            'armor': 0,\n",
    "            'distance': 0,\n",
    "        }\n",
    "        \n",
    "    def step(self, action, array=False):\n",
    "        state, reward, done, info = super().step(action)\n",
    "\n",
    "        if self.shaping:\n",
    "            shaped_reward = self.shape_rewards()\n",
    "        else:\n",
    "            shaped_reward = reward\n",
    "\n",
    "        self.state = state\n",
    "        self.total_rew += shaped_reward\n",
    "\n",
    "        return self.state, shaped_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._print_state()\n",
    "        \n",
    "        state = super().reset()\n",
    "\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.last_armor = 0\n",
    "        self.last_health = 100\n",
    "        self.last_frags = 0\n",
    "        self.total_rew = 0\n",
    "        self.deaths = 0\n",
    "\n",
    "        # Damage count  is not cleared when starting a new episode: https://github.com/mwydmuch/ViZDoom/issues/399\n",
    "        # self.last_damage_dealt = 0\n",
    "\n",
    "        # Reset reward stats\n",
    "        for k in self.rewards_stats.keys():\n",
    "            self.rewards_stats[k] = 0\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def shape_rewards(self):\n",
    "        reward_contributions = [\n",
    "            self._compute_frag_reward(),\n",
    "            self._compute_damage_reward(),\n",
    "            self._compute_ammo_reward(),\n",
    "            self._compute_health_reward(),\n",
    "            self._compute_armor_reward(),\n",
    "            self._compute_distance_reward(*self._get_player_pos()),\n",
    "        ]\n",
    "\n",
    "        return sum(reward_contributions)\n",
    "    \n",
    "     def _respawn_if_dead(self):\n",
    "        if not self.game.is_episode_finished():\n",
    "            # Check if player is dead\n",
    "            if self.game.is_player_dead():\n",
    "                self.deaths += 1\n",
    "                self._reset_player()\n",
    "\n",
    "    def _compute_distance_reward(self, x, y):\n",
    "        \"\"\"Computes a reward/penalty based on the distance travelled since last update.\"\"\"\n",
    "        dx = self.last_x - x\n",
    "        dy = self.last_y - y\n",
    "\n",
    "        self.last_x = x\n",
    "        self.last_y = y\n",
    "\n",
    "        distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "        d = distance - reward_threshold_distance\n",
    "\n",
    "        if d > 0:\n",
    "            reward = 5e-4\n",
    "        else:\n",
    "            reward = -5e-4\n",
    "\n",
    "        self._log_reward_stat('distance', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_frag_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total frags since last update.\"\"\"\n",
    "        frags = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        reward = reward_factor_frag * (frags - self.last_frags)\n",
    "\n",
    "        self.last_frags = frags\n",
    "        self._log_reward_stat('frag', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_damage_reward(self):\n",
    "        \"\"\"Computes a reward based on total damage inflicted to enemies since last update.\"\"\"\n",
    "        damage_dealt = self.game.get_game_variable(GameVariable.DAMAGECOUNT)\n",
    "        reward = reward_factor_damage * (damage_dealt - self.last_damage_dealt)\n",
    "\n",
    "        self.last_damage_dealt = damage_dealt\n",
    "        self._log_reward_stat('damage', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_health_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total health change since last update.\"\"\"\n",
    "        # When player is dead, the health game variable can be -999900\n",
    "        health = max(self.game.get_game_variable(GameVariable.HEALTH), 0)\n",
    "\n",
    "        health_reward = reward_factor_health_increment * max(0, health - self.last_health)\n",
    "        health_penalty = reward_factor_health_decrement * min(0, health - self.last_health)\n",
    "        reward = health_reward - health_penalty\n",
    "\n",
    "        self.last_health = health\n",
    "        self._log_reward_stat('health', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_armor_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total armor change since last update.\"\"\"\n",
    "        armor = self.game.get_game_variable(GameVariable.ARMOR)\n",
    "        reward = reward_factor_armor_increment * max(0, armor - self.last_armor)\n",
    "        \n",
    "        self.last_armor = armor\n",
    "        self._log_reward_stat('armor', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_ammo_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total ammunition change since last update.\"\"\"\n",
    "        self.weapon_state = self._get_weapon_state()\n",
    "\n",
    "        new_ammo_state = self._get_ammo_state()\n",
    "        ammo_diffs = (new_ammo_state - self.ammo_state) * self.weapon_state\n",
    "        ammo_reward = reward_factor_ammo_increment * max(0, np.sum(ammo_diffs))\n",
    "        ammo_penalty = reward_factor_ammo_decrement * min(0, np.sum(ammo_diffs))\n",
    "        reward = ammo_reward - ammo_penalty\n",
    "        \n",
    "        self.ammo_state = new_ammo_state\n",
    "        self._log_reward_stat('ammo', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _get_player_pos(self):\n",
    "        \"\"\"Returns the player X- and Y- coordinates\"\"\"\n",
    "        return self.game.get_game_variable(GameVariable.POSITION_X), self.game.get_game_variable(\n",
    "            GameVariable.POSITION_Y)\n",
    "\n",
    "    def _get_ammo_state(self):\n",
    "        \"\"\"Returns the total available ammunition per weapon slot.\"\"\"\n",
    "        ammo_state = np.zeros(10)\n",
    "\n",
    "        for i in range(10):\n",
    "            ammo_state[i] = self.game.get_game_variable(AMMO_VARIABLES[i])\n",
    "\n",
    "        return ammo_state\n",
    "\n",
    "    def _get_weapon_state(self):\n",
    "        \"\"\"Returns which weapon slots can be used. Available weapons are encoded as ones.\"\"\"\n",
    "        weapon_state = np.zeros(10)\n",
    "\n",
    "        for i in range(10):\n",
    "            weapon_state[i] = self.game.get_game_variable(WEAPON_VARIABLES[i])\n",
    "\n",
    "        return weapon_state\n",
    "\n",
    "    def _log_reward_stat(self, kind: str, reward: float):\n",
    "        self.rewards_stats[kind] += reward\n",
    "\n",
    "    def _reset_player(self):\n",
    "        self.last_health = 100\n",
    "        self.last_armor = 0\n",
    "        self.game.respawn_player()\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.ammo_state = self._get_ammo_state()\n",
    "\n",
    "    def _print_state(self):\n",
    "        super()._print_state()\n",
    "        print('Reward breakdown')\n",
    "        print('Agent {} frags: {}, deaths: {}, total reward: {}'.format(\n",
    "            self.name,\n",
    "            self.last_frags,\n",
    "            self.deaths,\n",
    "            self.total_rew\n",
    "        ))\n",
    "        for k, v in self.rewards_stats.items():\n",
    "            print(f'- {k}: {v:+.1f}')\n",
    "        print('*************************')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-lambda",
   "metadata": {},
   "source": [
    "We define some helper functions whose task is simply to create a VizDoom game instance and store it with our newly defined environment wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "figured-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv\n",
    "\n",
    "def game_instance(scenario):\n",
    "    # Create a VizDoom instance.\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config(f'scenarios/{scenario}.cfg')\n",
    "    game.add_game_args('-host 1 -deathmatch +viz_nocheat 0 +cl_run 1 +name AGENT +colorset 0' +\n",
    "                       '+sv_forcerespawn 1 +sv_respawnprotect 1 +sv_nocrouch 1 +sv_noexit 1')\n",
    "    game.init()\n",
    "    \n",
    "    return game\n",
    "\n",
    "def env_with_bots_shaped(scenario, **kwargs) -> envs.DoomEnv:\n",
    "    game = game_instance(scenario)\n",
    "    return DoomWithBotsShaped(game, **kwargs)\n",
    "\n",
    "def vec_env_with_bots_shaped(n_envs=1, **kwargs) -> VecTransposeImage:\n",
    "    return VecTransposeImage(DummyVecEnv([lambda: env_with_bots_shaped(**kwargs)] * n_envs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-wallace",
   "metadata": {},
   "source": [
    "We can now train on the map introduced in the second part. The code loading the model, registering the callbacks and starting the learning process has been moved to the `common` module for readability. This way, we can start the training with a single call to `solve_env` which will handle the aspects we have already covered previously.\n",
    "\n",
    "In the part below we define:\n",
    "\n",
    "* A frame processor that crops and resizes raw game frames.\n",
    "* Environment parameters such as how much frame skipping, how many bots etc.\n",
    "* Agent parameters such as the learning rate, steps per rollout and our custom CNN created in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "threaded-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.models import CustomCNN\n",
    "from common.envs import solve_env\n",
    "\n",
    "scenario = 'deathmatch_simple'\n",
    "\n",
    "# Results in a 100x156 image, no pixel lost due to padding with our CNN architecture.\n",
    "frame_processor = lambda frame: cv2.resize(frame[40:, 4:-4], None, fx=.5, fy=.5, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Environment parameters.\n",
    "env_args = {\n",
    "    'scenario': scenario,\n",
    "    'frame_skip': 4,\n",
    "    'frame_processor': frame_processor,\n",
    "    'n_bots': 8,\n",
    "    'shaping': True\n",
    "}\n",
    "\n",
    "# In the evaluation environment we measure frags only.\n",
    "eval_env_args = dict(env_args)\n",
    "eval_env_args['shaping'] = False\n",
    "\n",
    "# Agent parameters.\n",
    "agent_args = {\n",
    "    'n_epochs': 3,\n",
    "    'n_steps': 4096,\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': 32,\n",
    "    'policy_kwargs': {'features_extractor_class': CustomCNN}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ranging-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environments with bots and shaping.\n",
    "#env = vec_env_with_bots_shaped(2, **env_args)\n",
    "#eval_env = vec_env_with_bots_shaped(1, **eval_env_args)\n",
    "\n",
    "#solve_env(env, eval_env, scenario, agent_args)\n",
    "\n",
    "#env.close()\n",
    "#eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-string",
   "metadata": {},
   "source": [
    "You should see some noticeable improvement over our previous setup with average rewards starting to rise much earlier in the learning process. In the figure below I have illustrated the average reward curve over 6 consecutive trials. The coloured region denotes the error mean: $ \\frac{\\sigma}{\\sqrt{n}}$. The difference is stunning! Within the same amount of time we were able to obtain a 3x improvement! In addition, we see that our agent is already stronger than programmed bots!\n",
    "\n",
    "Reward shaping is not the only way of improving the learning performance. In the next part of this notebook we will see how curriculum learning can further boost our learning performance.\n",
    "\n",
    "![Comparison performance](./figures/comparison_shaping_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-miller",
   "metadata": {},
   "source": [
    "<div id=part_2></div>\n",
    "\n",
    "# [^](#top) Part 2 - Curriculum Learning\n",
    "\n",
    "The concept behind curriculum learning is to make the learning task easy at first and then gradually increase the difficulty as the agent progresses. To implement the idea in a deathmatch environment we will alter the speed and health of bots based on the performance of the agent. The following table summarises the bots parameters based on the average reward obtained by the agent. The average reward is computed over the last 10 episodes.\n",
    "\n",
    "| Average reward over last 10 episodes | Bot multiplier |\n",
    "| :----------------------------------: |:--------------:| \n",
    "| <5                                   |  0.1           |\n",
    "| <10                                  |  0.2           |\n",
    "| <15                                  |  0.4           |\n",
    "| <20                                  |  0.6           |\n",
    "| <25                                  |  0.8           |\n",
    "| >25                                  |  1.0           |\n",
    "\n",
    "We can't directly influence the behaviour of bots with Python code. Instead, we need to use ACS scripts. Those scripts are stored in the `.wad` file alongside each map. To read and edit scripts, you can use [Slade](https://slade.mancubus.net/) Using ACS, it is quite easy to modify game variables. The following snippet is all we need for the task. For more information about ACS, you can refer to [ZDoom ACS documentation](https://zdoom.org/wiki/ACS).\n",
    "\n",
    "\n",
    "<div id=acs_script></div>\n",
    "\n",
    "### ACS Script:\n",
    "---\n",
    "```C\n",
    "#include \"zcommon.acs\"\n",
    "\n",
    "global int 0:reward;\n",
    "\n",
    "int difficulty_level = 5;\n",
    "int speed_levels[6] = {0.1, 0.2, 0.4, 0.6, 0.8, 1.0};\n",
    "int health_levels[6] = {10, 20, 40, 60, 80, 100};\n",
    "\n",
    "script 1 OPEN\n",
    "{\n",
    "  Log(s:\"Level loaded\");\n",
    "}\n",
    "\n",
    "script 2 ENTER\n",
    "{\n",
    "  set_actor_skill(ActivatorTID());\n",
    "}\n",
    "\n",
    "script 3 RESPAWN\n",
    "{\n",
    "  set_actor_skill(ActivatorTID());\n",
    "}\n",
    "\n",
    "script \"change_difficulty\" (int new_difficulty_level)\n",
    "{\n",
    "  Log(s:\"Changing difficulty level to: \", d: new_difficulty_level);\n",
    "  \n",
    "  difficulty_level = new_difficulty_level;\n",
    "}\n",
    "\n",
    "function void set_actor_skill(int actor_id)\n",
    "{\n",
    "  if (ClassifyActor(actor_id) & ACTOR_BOT ) {\n",
    "    Log(s:\"Changing difficulty level for bot!\", d: actor_id, d: difficulty_level);\n",
    "    SetActorProperty(actor_id, APROP_Speed , speed_levels[difficulty_level]);\n",
    "    SetActorProperty(actor_id, APROP_Health , health_levels[difficulty_level]);\n",
    "  }\n",
    "}\n",
    "```\n",
    "---\n",
    "<div id=curriculum_env></div>\n",
    "\n",
    "## Curriculum environment wrapper\n",
    "To interact with a function defined in an ACS script we can use the `puke` and `pukename` commands (the latter allows calling function by their name).\n",
    "\n",
    "```Python\n",
    "game.send_game_command(f'pukename <function name> <arguments>')\n",
    "```\n",
    "\n",
    "For more details, have a look at the ZDoom Wiki. Just like for reward shaping, we will subclass the environment wrapper to add the behaviour we need. Also, we need to make sure that the environment used for evaluating the agent's performance is using the normal difficulty (no curriculum applied). Otherwise we would have biased estimates of our agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unnecessary-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "REWARD_THRESHOLDS = [5, 10, 15, 20, 25, 25]\n",
    "\n",
    "class DoomWithBotsCurriculum(DoomWithBotsShaped):\n",
    "\n",
    "    def __init__(self, game, frame_processor, frame_skip, n_bots, shaping, initial_level=0, max_level=5, rolling_mean_length=10):\n",
    "        super().__init__(game, frame_processor, frame_skip, n_bots, shaping)\n",
    "        \n",
    "        # Initialize ACS script difficulty level\n",
    "        game.send_game_command('pukename change_difficulty 0')\n",
    "        \n",
    "        # Internal state\n",
    "        self.level = initial_level\n",
    "        self.max_level = max_level\n",
    "        self.rolling_mean_length = rolling_mean_length\n",
    "        self.last_rewards = deque(maxlen=rolling_mean_length)\n",
    "\n",
    "    def step(self, action, array=False):\n",
    "        state, reward, done, infos = super().step(action, array)\n",
    "\n",
    "        # After an episode, check whether difficulty should be increased.\n",
    "        if done:\n",
    "            self.last_rewards.append(self.total_rew)\n",
    "            run_mean = np.mean(self.last_rewards)\n",
    "            print('Avg. last 10 runs of {}: {:.2f}. Current difficulty level: {}'.format(self.name, run_mean, self.level))\n",
    "            if run_mean > REWARD_THRESHOLDS[self.level] and len(self.last_rewards) >= self.rolling_mean_length:\n",
    "                self._change_difficulty()\n",
    "\n",
    "        return state, reward, done, infos\n",
    "\n",
    "    def reset(self):\n",
    "        state = super().reset()\n",
    "        self.game.send_game_command(f'pukename change_difficulty {self.level}')\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _change_difficulty(self):\n",
    "        \"\"\"Adjusts the difficulty by setting the difficulty level in the ACS script.\"\"\"\n",
    "        if self.level < self.max_level:\n",
    "            self.level += 1\n",
    "            print(f'Changing difficulty for {self.name} to {self.level}')\n",
    "            self.game.send_game_command(f'pukename change_difficulty {self.level}')\n",
    "            self.last_rewards = deque(maxlen=self.rolling_mean_length)\n",
    "        else:\n",
    "            print(f'{self.name} already at max level!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-criterion",
   "metadata": {},
   "source": [
    "Finally, we launch a training session using the wrapper for curriculum learning and wait for 3M steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reflected-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_with_bots_curriculum(scenario, **kwargs) -> envs.DoomEnv:\n",
    "    game = game_instance(scenario)\n",
    "    return DoomWithBotsCurriculum(game, **kwargs)\n",
    "\n",
    "def vec_env_with_bots_curriculum(n_envs=1, **kwargs) -> VecTransposeImage:\n",
    "    return VecTransposeImage(DummyVecEnv([lambda: env_with_bots_curriculum(**kwargs)] * n_envs))\n",
    "\n",
    "# Create environments with bots.\n",
    "#env = vec_env_with_bots_curriculum(2, **env_args)\n",
    "#eval_env = vec_env_with_bots_shaped(1, **eval_env_args) # Don't use adaptive curriculum for the evaluation env!\n",
    "\n",
    "#solve_env(env, eval_env, agent_args)\n",
    "\n",
    "#env.close()\n",
    "#eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-boards",
   "metadata": {},
   "source": [
    "You should now see an even better performance than before. By combining reward shaping and curriculum we managed to get a 4x increase in performance for the same number of training steps! We are now fully equipped to efficiently train an agent that will outmatch even the best programmed bot beyond a shadow of a doubt. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-yugoslavia",
   "metadata": {},
   "source": [
    "![Comparison performance](./figures/comparison_shaping_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-gather",
   "metadata": {},
   "source": [
    "<div id=final_model></div>\n",
    "\n",
    "## Final model\n",
    "\n",
    "To celebrate the training of the final model, I have created a fancier map that requires efforts for players to navigate and find enemies. The screenshot below shows an overhead view of the new map. You can find the corresponding `.wad` file on the GitHub repository.\n",
    "\n",
    "![New map](./figures/map_2_scaled.png)\n",
    "\n",
    "To obtain the final model, I trained with reward shaping and curriculum learning as follows:\n",
    "\n",
    "1. 10M training steps from scratch using a frame skip parameter of 4.\n",
    "1. 10M training steps using the previous result and setting the frame skip parameter to 2.\n",
    "1. 10M training steps using the previous result and setting the frame skip parameter to 1.\n",
    "\n",
    "At the beginning of the training process, the frame skip is set relatively high to speed up the learning. Then, it is progressively reduced to improve the aiming accuracy of the agent. The figure below shows the final \"learning curve\" for this setup. Notice the sharp jump in performance as soon as we allow the agent to skip less frames.\n",
    "\n",
    "![Best model training](./figures/final_training_rewards.png)\n",
    "\n",
    "Here is an animation of resulting agent in action, destroying the competition:\n",
    "\n",
    "![Final agent](./figures/deathmatch_stack=4.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-eagle",
   "metadata": {},
   "source": [
    "<div id=\"bonus\"></div>\n",
    "\n",
    "# Bonus: play against your agent!\n",
    "\n",
    "There are two helper scripts in the `bin` folder:\n",
    "\n",
    "* `demo_deathmatch.sh`\n",
    "* `demo_multiplayer.sh`\n",
    "\n",
    "The first one starts a game of deathmatch with 8 bots and a pretrained agent. Use it if you want a demonstration of what the agent can do. The second one will spawn two instances of Doom, one for a human player and one for the pretrained agent. Each player joins the same deathmatch game with 7 programmed bots. See how your skills compare to the AI! Good luck! \n",
    "\n",
    "Note: Due to the limitation of Jupyter notebooks to run multiple processes in parallel, I could not include the demo directly in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-cigarette",
   "metadata": {},
   "source": [
    "<div id=\"conclusion\"></div>\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Over the course of this three-part series we managed to train a reinforcement learning agent to play Doom deathmatch games. We started with a basic setup able to solve very simple scenarios where only a limited number of actions were allowed and where the complexity of the learning task was significantly constrained. We worked our way towards more elaborated tasks by increasing the number of parameters in our model and monitoring closely the learning process to ensure a smooth progression. Finally, we saw how reward shaping and curriculum learning could boost the training to reach good result much quicker.\n",
    "\n",
    "\n",
    "## Potential improvements\n",
    "If you have played a game of deathmatch against a fully trained agent you will have noticed that it is actually quite hard to keep up in terms of score. However, when playing in a 1 versus 1 it remains quite easy to defeat it due to its overall lack of strategy. I've identified three aspects that can be improved:\n",
    "\n",
    "### Memory\n",
    "You might have noticed that the agent has no concept of memory. This means that ennemies that are not visible on the screen are immediately forgotten by the agent. Also, the agent does not keep track of places that it has already visied. This is not a big issue when playing against 8 programmed bots as there is always an enemy close by. However, when playing against a single opponent this means that the agent will revisit several times the same location of simply ignore some area of the map it should have explored.\n",
    "\n",
    "A potential improvement here would be to use a model that has a concept of memory like a LSTM neural network. This paper shows that such a model could be used to play Doom effectively: Lample, Guillaume, and Devendra Singh Chaplot. “Playing FPS Games with Deep Reinforcement Learning.” ArXiv:1609.05521 [Cs], Jan. 2018. arXiv.org, http://arxiv.org/abs/1609.05521.\n",
    "\n",
    "### Difficulty\n",
    "If you have played yourself, you might have noticed that the programmed bots are not the smartest of opponents. They will often get stuck against walls or randomly run across the map. This also means that the amount of strategy needed by our agent to get good rewards is limited. It might be interesting to see wether we can increase the performance of the agent by letting it play against versions of itself. Stronger opponents means the agent will potentially learn more interesting strategies.\n",
    "\n",
    "### Agressivity\n",
    "The agent prefers attacking than picking strategic items or protecting himself. It is hard to point to a single cause but it might be possible to mitigate this behaviour by picking different weights for the reward shaping process or defining new actions to be reinforced altogether.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
